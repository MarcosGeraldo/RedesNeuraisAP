{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "c4HO0",
      "launcher_item_id": "lSYZM"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Lab3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DOzgbH4-Lfl3",
        "4FN1QQO4LfmB"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl-F1Cp0cZuY"
      },
      "source": [
        "# Lab 3 - BCC406\n",
        "\n",
        "## REDES NEURAIS E APRENDIZAGEM EM PROFUNDIDADE\n",
        "\n",
        "## Construindo uma rede neural\n",
        "\n",
        "### Prof. Eduardo e Prof. Pedro Silva\n",
        "\n",
        "Data da entrega : 15/04 \n",
        "\n",
        "- Complete o código (marcado com ToDo) e quando requisitado, escreva textos diretamente nos notebooks. Onde tiver *None*, substitua pelo seu código.\n",
        "- Execute todo notebook e salve tudo em um PDF **nomeado** como \"NomeSobrenome-Lab3.pdf\"\n",
        "- Envie o PDF para pelo [FORM](https://forms.gle/LdGDeFYH6wQm9ahh6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1TpbxeTLfkU"
      },
      "source": [
        "# **Parte 1** - Rede neural do zero: passo a passo (10pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notação**:\n",
        "- Sobrescrito índice $ [l] $ indica os valores associados a $l$-ésima camada.\n",
        "    - **Exemplo:** $ a^{[l]} $ é a ativação da $l$-ésima camada.\n",
        "- Sobrescrito índice $ (i) $ indica os valores associados ao $i$-ésima exemplo.\n",
        "    - **Exemplo:** $ x^{(i)} $ é o  $i$-ésima exemplo de treinamento.\n",
        "- Subescrito índice $ j $ indica a $j$-ésima entrada de um vetor.\n",
        "    - **Exemplo:** $a^{[l]}_j$ indica a $j$-ésima  entrada  da ativação da $ l$-ésima camada."
      ],
      "metadata": {
        "id": "H_ltW_dGP5MU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Importação dos pacotes\n"
      ],
      "metadata": {
        "id": "rTcwU5csP-Px"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAOVC8TLLfkV"
      },
      "source": [
        "Primeiro, vamos executar a célula abaixo para importar todos os pacotes que precisaremos.\n",
        "- [numpy](www.numpy.org) é o pacote fundamental para a computação científica com Python.\n",
        "- [h5py](http://www.h5py.org) é um pacote comum para interagir com um conjunto de dados armazenado em um arquivo H5.\n",
        "- [matplotlib](http://matplotlib.org) é uma biblioteca famosa para plotar gráficos em Python.\n",
        "- [PIL](http://www.pythonware.com/products/pil/) e [scipy](https://www.scipy.org/) são usados aqui para testar seu modelo.\n",
        "- dnn_utils fornece algumas funções necessárias para este notebook.\n",
        "- testCases fornece alguns casos de teste para avaliar as funções.\n",
        "- np.random.seed (1) é usado para manter todas as chamadas de funções aleatórias. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9BlvNrOLl3M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0c78f962-36be-43ac-8e7d-7a35ed4d15f8"
      },
      "source": [
        "# Para Google Colab: Você vai precisar fazer o upload dos arquivos no seu drive e montá-lo\n",
        "# não se esqueça de ajustar o path para o seu drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEWo1hJ4TpN9"
      },
      "source": [
        "# Você vai precisar inserir seu diretório para importar as \"bibliotecas próprias\" auxiliares deste notebook\n",
        "# não se esqueça de ajustar o path para o seu diretório\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne48u9hjLfkX"
      },
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "# bibliotecas auxiliares (ver testCases_v4a.py e dnn_utils_v2.py)\n",
        "from testCases_v4a import *\n",
        "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
        "#\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jF5eYx4Lfkc"
      },
      "source": [
        "## 2 - Esboço das Funções auxiliares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "- Inicialização dos parâmetros da rede.\n",
        "- Implementação da fase forward propagation (roxo na figura abaixo).\n",
        "     - Complete a parte LINEAR da etapa de forward propagation de uma camada (resultando em $ Z^{[l]}$).\n",
        "     - Fornecemos a função ATIVAÇÃO (relu / sigmóide).\n",
        "     - Combine os dois passos anteriores em uma nova função de avanço [LINEAR-> ATIVAÇÃO].\n",
        "     - Empilhe a função de avanço [LINEAR-> RELU] L-1 (para as camadas 1 a L-1) e adicione um [LINEAR-> SIGMOID] no final (para a camada final $ L $). Isso fornece uma nova função L_model_forward.\n",
        "- Cálculo a função loss.\n",
        "- Implementação da fase backward propagation (vermelho na figura abaixo).\n",
        "    - Complete a parte LINEAR da etapa de backward propagation de uma camada.\n",
        "    - Fornecemos o gradiente da função (relu_backward / sigmoid_backward)\n",
        "    - Combine as duas etapas anteriores em uma nova função [LINEAR-> ATIVAÇÃO] para trás.\n",
        "    - Empilhe [LINEAR-> RELU] para trás L-1 vezes e adicione [LINEAR-> SIGMOID] para trás em uma nova função L_model_backward\n",
        "- Atualização dos parâmetros.\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19iowxbfZWLXFvB6eGP0hcRZcsQFltmHk)\n",
        "\n",
        "<caption><center> **Figura 1**</center></caption><br>\n",
        "\n",
        "**Observe** que para todas as etapas forward, existe uma etapa backward correspondente. É por isso que em cada etapa forward você estará armazenando alguns valores em cache. Os valores em cache são úteis para calcular gradientes. Na etapa backward, você usará o cache para calcular os gradientes. "
      ],
      "metadata": {
        "id": "oXNJeQVVPyA1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSEMNMlOLfkd"
      },
      "source": [
        "## 3 - Inicialização (1pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A função será usada para inicializar parâmetros para uma rede com $ L $-camadas.\n",
        "\n",
        "### 3.1 - Rede Neural com $L$-camadas\n",
        "\n",
        "**Instruções**:\n",
        "- A estrutura do modelo é * [LINEAR -> RELU] $ \\times $ (L-1) -> LINEAR -> SIGMOID *. Ou seja, possui (L-1) camadas  usando uma função de ativação ReLU seguida por uma camada de saída com uma função de ativação sigmóide.\n",
        "- Use inicialização aleatória para as matrizes de peso. Use `np.random.randn(shape) * 0,01`.\n",
        "- Use a inicialização de zeros para os vieses. Use `np.zeros(shape)`.\n",
        "- Armazenaremos $ n ^ {[l]} $, o número de elementos/neurônios na camada $l$, em uma variável `camadas_dims`. Por exemplo, `camadas_dims = [2,4,1]` é uma rede com duas entradas, uma camada oculta com 4 unidades/neurônios e uma camada de saída com 1 unidade/neurônio de saída . "
      ],
      "metadata": {
        "id": "Zm-gglS4Psce"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL745AoeLfke"
      },
      "source": [
        "# Inicialize_parametros\n",
        "\n",
        "def inicialize_parametros(camadas_dims):\n",
        "    \"\"\"\n",
        "    Entrada:\n",
        "    camadas_dims -- python array (lista) contendo a dimensão de cada camada da rede\n",
        "    \n",
        "    \n",
        "    Saída:\n",
        "    parametros   -- python dicionario contendo os parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- vetor de pesos com formato (camadas_dims[l], camadas_dims[l-1])\n",
        "                    bl -- vetor de vies com formato (camadas_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parametros = {}\n",
        "    L = np.size(camadas_dims)            # ToDo: número de camadas da rede\n",
        "\n",
        "    ### Início do código ### \n",
        "    for l in range(1, L):\n",
        "      # dica: itere pelo número de camadas, inicializando pesos e viés de cada camada,\n",
        "      # e armazenem em parameters (≈ 2 linhas de código)\n",
        "      parametros['W' + str(l)] = np.random.randn(camadas_dims[l], camadas_dims[l-1]) * 0.01 # ToDo   \n",
        "      parametros['b' + str(l)] = np.zeros((camadas_dims[l], 1)) # ToDo    \n",
        "    ### Fim do código ###\n",
        "        \n",
        "    return parametros"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HPweLSNLfkh"
      },
      "source": [
        "**Comentários** \n",
        "Ao concluir o `inicialize_parametros`, certifique-se de que as dimensões entre cada camada estejam corretas. Lembre-se de que $ n ^ {[l]} $ é o número de unidades na camada $ l $. Assim, por exemplo, se o tamanho da nossa entrada $ X $ for $ (12288, 209) $ (com número de exemplos $ m = 209 $), então:\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "\n",
        "\n",
        "   <tr>\n",
        "        <td>  </td> \n",
        "        <td> **Formato de W** </td> \n",
        "        <td> **Formato de b**  </td> \n",
        "        <td> **Ativação** </td>\n",
        "         <td> **Formato da Ativação** </td> \n",
        "    <tr>\n",
        "    \n",
        "   <tr>\n",
        "        <td> **Camada 1** </td> \n",
        "        <td> $(n^{[1]},12288)$ </td> \n",
        "        <td> $(n^{[1]},1)$ </td> \n",
        "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td>         \n",
        "        <td> $(n^{[1]},209)$ </td> \n",
        "    <tr>\n",
        "    \n",
        "   <tr>\n",
        "        <td> **Camada 2** </td> \n",
        "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
        "        <td> $(n^{[2]},1)$ </td> \n",
        "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
        "        <td> $(n^{[2]}, 209)$ </td> \n",
        "    <tr>\n",
        "   \n",
        "   <tr>\n",
        "        <td> $\\vdots$ </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$</td> \n",
        "        <td> $\\vdots$  </td> \n",
        "    <tr>\n",
        "    \n",
        "   <tr>\n",
        "        <td> **Camada L-1** </td> \n",
        "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
        "        <td> $(n^{[L-1]}, 1)$  </td> \n",
        "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
        "        <td> $(n^{[L-1]}, 209)$ </td> \n",
        "    <tr>\n",
        "    \n",
        "    \n",
        "   <tr>\n",
        "        <td> **Camada L** </td> \n",
        "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
        "        <td> $(n^{[L]}, 1)$ </td>\n",
        "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
        "        <td> $(n^{[L]}, 209)$  </td> \n",
        "    <tr>\n",
        "\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9iMuR52Lfkh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "331eab8a-7555-4b15-8eb2-9c129dc70fa8"
      },
      "source": [
        "# Teste\n",
        "parametros = inicialize_parametros([5,4,3])\n",
        "print(\"W1 = \" + str(parametros[\"W1\"]))\n",
        "print(\"b1 = \" + str(parametros[\"b1\"]))\n",
        "print(\"W2 = \" + str(parametros[\"W2\"]))\n",
        "print(\"b2 = \" + str(parametros[\"b2\"]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
            " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
            " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
            " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
            "b1 = [[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
            " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
            " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
            "b2 = [[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPW7fW79Lfkl"
      },
      "source": [
        "**Valores esperados**:\n",
        "       \n",
        "<table style=\"width:80%\">\n",
        "  <tr>\n",
        "    <td> **W1** </td>\n",
        "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
        " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
        " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
        " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b1** </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**W2** </td>\n",
        "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
        " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
        " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
        "  </tr>\n",
        "  \n",
        "  <tr>\n",
        "    <td>**b2** </td>\n",
        "    <td>[[ 0.]\n",
        " [ 0.]\n",
        " [ 0.]]</td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mbb4pJFaLfkl"
      },
      "source": [
        "## 4 - Fase: Forward propagation (2pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Usaremos duas funções:\n",
        "- LINEAR\n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID\n",
        "\n",
        "### 4.1 - Linear Forward \n",
        "\n",
        "A função linear_forward (sobre todos os examples) é definida pela equação:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "onde $A^{[0]} = X$. \n",
        "\n",
        "\n",
        "**Lembrete**\n",
        "Lembre-se de que quando calculamos $ W X + b $ em python, ele realiza `broadcasting`. Por exemplo, se:\n",
        "$$ W = \\begin{bmatrix}\n",
        "    j  & k  & l\\\\\n",
        "    m  & n & o \\\\\n",
        "    p  & q & r \n",
        "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
        "    a  & b  & c\\\\\n",
        "    d  & e & f \\\\\n",
        "    g  & h & i \n",
        "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
        "    s  \\\\\n",
        "    t  \\\\\n",
        "    u\n",
        "\\end{bmatrix}\\tag{2}$$\n",
        "\n",
        "Então $WX + b$ será:\n",
        "\n",
        "$$ WX + b = \\begin{bmatrix}\n",
        "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
        "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
        "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
        "\\end{bmatrix}\\tag{3}  $$"
      ],
      "metadata": {
        "id": "WzsLpjbHPhjY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2zuS4HdLfkn"
      },
      "source": [
        "# Função linear_forward\n",
        "\n",
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implementa a parte linear da fase de propogação nas camadas\n",
        "\n",
        "    Entradas:\n",
        "    A - dados de entrada da camada atual (ativações da camada anterior): formato (tamanho da camada anterior, número de exemplos)\n",
        "    W - matriz de pesos: matriz numpy com formato (tamanho da camada atual, tamanho da camada anterior)\n",
        "    b - vetor de viés, matriz numpy com formato (tamanho da camada atual, 1)\n",
        "\n",
        "    Saídas:\n",
        "    Z -- a entrada da função de ativação, também chamada de parâmetro de pré-ativação\n",
        "    cache - uma tupla python contendo \"A\", \"W\" e \"b\"; (armazenado para usar na fase backward propagation)\n",
        "    \"\"\"\n",
        "    \n",
        "    ### Início do código ### (≈ 2 linhas de código)\n",
        "    Z = np.dot(W,A)+b # dica: use a funçao .dot() \n",
        "    cache = (A, W, b)\n",
        "    ### Fim do código ###\n",
        "\n",
        "    \n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yaYF_aMLfkr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4743a99c-16b7-41e9-ee05-2908b5bbe196"
      },
      "source": [
        "# Teste\n",
        "A, W, b = linear_forward_test_case()\n",
        "\n",
        "Z, linear_cache = linear_forward(A, W, b)\n",
        "\n",
        "print(\"Z \" + str(Z))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Z [[ 3.26295337 -1.23429987]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0zOgOn1Lfku"
      },
      "source": [
        "**Valores Esperados**:\n",
        "\n",
        "<table style=\"width:35%\">\n",
        "  \n",
        "  <tr>\n",
        "    <td> **Z** </td>\n",
        "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
        "  </tr>\n",
        "  \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ky5XExaLfkv"
      },
      "source": [
        "### 4.2 - Linear-Ativação Forward\n",
        "\n",
        "Usaremos duas funções de ativação:\n",
        "\n",
        "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. A função `sigmoid`, **retorna dois** itens: o valor de ativação \"` a` \"e um\" `cache`\" que contém \"` Z` \"(necessário para a fase backward correspondente). Para usá-lo, basta chamar: \n",
        "``` python\n",
        "A, ativacao_cache = sigmoid(Z)\n",
        "```\n",
        "\n",
        "- **ReLU**: A formula é $A = RELU(Z) = max(0, Z)$. A função `relu`, **retorna dois** itens: o valor de ativação \"` a` \"e um\" `cache`\" que contém \"` Z` \"(necessário para a fase backward correspondente). Para usá-lo, basta chamar:\n",
        "``` python\n",
        "A, ativacao_cache = relu(Z)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yoy96_gLfkv"
      },
      "source": [
        "**Exercício**: Implemente a *LINEAR-> ATIVAÇÃO* da camada da fase forward propagation. A relação matemática é: $ A^{[l]} = g (Z^{[l]}) = g (W^{[l]} A^{[l-1]} + b^{[l]} ) $ onde a ativação \"g\" pode ser `sigmoid` ou `relu`. Use a função linear_forward ()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ia-RStILfkw"
      },
      "source": [
        "# Função linear_ativacao_forward\n",
        "\n",
        "def linear_ativacao_forward(A_prev, W, b, ativacao):\n",
        "    \"\"\"\n",
        "    Implementa a *LINEAR-> ATIVAÇÃO* da camada da fase forward propagation\n",
        "\n",
        "    Entradas:\n",
        "    A_prev -- dados de entrada da camada atual (ativações da camada anterior): formato (tamanho da camada anterior, número de exemplos)\n",
        "    W - matriz de pesos: matriz numpy com formato (tamanho da camada atual, tamanho da camada anterior)\n",
        "    b - vetor de viés, matriz numpy com formato (tamanho da camada atual, 1)\n",
        "    ativacao -- \"sigmoid\" ou \"relu\"\n",
        "\n",
        "    Saídas:\n",
        "    A -- a saída da função de ativação, também chamada de valor da pós-ativação\n",
        "    cache -- uma tupla python contendo \"linear_cache\" e \"ativacao_cache\"; \n",
        "    (armazenado para usar na fase backward propagation)          \n",
        "    \"\"\"\n",
        "    \n",
        "    if ativacao == \"sigmoid\":\n",
        "        # Entradas: \"A_prev, W, b\". Saídas: \"A, ativacao_cache\".\n",
        "        ### Início do código ###\n",
        "        # dicas: use sua funcao de propagação e as funções de ativação fornecidas em dnn_utils_v2.py\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = sigmoid(Z)\n",
        "        ### Fim do código ###\n",
        "    \n",
        "    elif ativacao == \"relu\":\n",
        "        # Entradas: \"A_prev, W, b\". Saídas: \"A, ativacao_cache\".\n",
        "        ### Início do código ### \n",
        "        # dicas: use sua funcao de propagação e as funções de ativação fornecidas em dnn_utils_v2.py\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "        ### Fim do código ###\n",
        "\n",
        "    cache= (linear_cache,activation_cache)\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_AkFIuvLfk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c900c6b0-965e-4782-c94e-8b3d20a36547"
      },
      "source": [
        "# Teste\n",
        "\n",
        "A_prev, W, b = linear_activation_forward_test_case()\n",
        "\n",
        "A, linear_ativacao_cache = linear_ativacao_forward(A_prev, W, b, ativacao = \"sigmoid\")\n",
        "print(\"com sigmoid: A = \" + str(A))\n",
        "\n",
        "A, linear_ativacao_cache = linear_ativacao_forward(A_prev, W, b, ativacao = \"relu\")\n",
        "print(\"com ReLU: A = \" + str(A))\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "com sigmoid: A = [[0.96890023 0.11013289]]\n",
            "com ReLU: A = [[3.43896131 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzUZyUGZLfk3"
      },
      "source": [
        "**Valores esperados**:\n",
        "       \n",
        "<table style=\"width:35%\">\n",
        "  <tr>\n",
        "    <td> **com sigmoid: A ** </td>\n",
        "    <td > [[ 0.96890023  0.11013289]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **com ReLU: A ** </td>\n",
        "    <td > [[ 3.43896131  0.        ]]</td> \n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0xQWVgsLfk4"
      },
      "source": [
        "### d) Modelo de L-camadas\n",
        "\n",
        "Replica a função `linear_ativacao_forward` com RELU $(L-1)$ vezes, depois uma vez `linear_ativacao_forward` com SIGMOID.\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19bhG0GkUjw3fLBTmoKjDZImWcq2nE2GD)\n",
        "\n",
        "\n",
        "<caption><center> **Figura 2** : Esquema do modelo *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* </center></caption><br>\n",
        "\n",
        "**Instrução**: A variável `AL` é $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (ativaçnao da última camada, i.e., $\\hat{Y}$.) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz4OPSekLfk4"
      },
      "source": [
        "# L_modelo_forward\n",
        "\n",
        "def L_modelo_forward(X, parametros):\n",
        "    \"\"\"\n",
        "    Implementa a fase forward propagation \n",
        "    \n",
        "    Entradas:\n",
        "    X -- dados, numpy array de tamanho (input size, number of examples)\n",
        "    parametros -- parametros iniciais\n",
        "    \n",
        "    Saídas:\n",
        "    AL -- valor da pós-ativação da última camada\n",
        "    caches -- lista dos caches contendo:\n",
        "                todos caches da linear_ativacao_forward() (existem L-1 deles, indexados de 1 a L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X                  # dados da camada inicial\n",
        "    L = int((len(parametros))/2)         # números de camadas da rede\n",
        "    \n",
        "    # Implemente [LINEAR -> RELU]*(L-1). Adicione o \"linear_cache\" para a lista \"caches\".\n",
        "    ### Início do código ###\n",
        "    for l in range(1,L):\n",
        "        A_prev = A   \n",
        "        A, cache = linear_ativacao_forward(A_prev, parametros[\"W\"+str(l)], parametros[\"b\"+str(l)], ativacao = \"relu\") # dica: utilize linear_activation_forward()\n",
        "        caches.append(cache)\n",
        "    ### Fim do código ###\n",
        "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    ### Início do código ### \n",
        "    AL, cache = linear_ativacao_forward(A, parametros[\"W\"+str(L)], parametros[\"b\"+str(L)], ativacao = \"sigmoid\") # dica : utilize linear_activation_forward()\n",
        "    #AL=sum(AL)\n",
        "    caches.append(cache)\n",
        "    ### Fim do código ###\n",
        "\n",
        "    \n",
        "    \n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mK30kYjLfk7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ffc18202-50ef-4f35-b1b7-e7d65c7f10a9"
      },
      "source": [
        "X, parametros = L_model_forward_test_case_2hidden()\n",
        "AL, caches = L_modelo_forward(X, parametros)\n",
        "\n",
        "print(\"AL = \" + str(AL))\n",
        "print(\"Tamanho da lista caches = \" + str(len(caches)))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
            "Tamanho da lista caches = 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CG1VCWELfk_"
      },
      "source": [
        "**Valores Esperados:**\n",
        "\n",
        "<table style=\"width:50%\">\n",
        "  <tr>\n",
        "    <td> **AL** </td>\n",
        "    <td > [[ 0.03921668  0.70498921  0.19734387  0.04728177]]</td> \n",
        "  </tr>\n",
        "  <tr>\n",
        "    <td> **Tamanho da lista caches ** </td>\n",
        "    <td > 3 </td> \n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qil7E18JLfk_"
      },
      "source": [
        "Usando $A^{[L]}$, você deve calcular o custo da rede."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiqOiRqvLflC"
      },
      "source": [
        "## 5 - Função Custo (cross-entropy) (2pt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para a fase backward propagation é necessário o cálculo da funcão custo.\n",
        "\n",
        "**Exercício**: Use a seguinte função custo: $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left(y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)\\right) \\tag{7}$$\n",
        "\n",
        "obs.: veja que é a mesma implementada para o Lab1b."
      ],
      "metadata": {
        "id": "rBGpvFlFParB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UDSELo2LflD"
      },
      "source": [
        "# Função custo\n",
        "\n",
        "def custo(AL, Y):\n",
        "    \"\"\"\n",
        "    Implementa a função custo da rede.\n",
        "\n",
        "    Entradas:\n",
        "    AL -- Probabiliade de predição da rede, (1, numero de exemplos)\n",
        "    Y -- Vetor de rótulos dos exemplos de treinamento  ( 0 se não tem gato, 1 tem gato ), (1, numero de exemplos)\n",
        "\n",
        "    Saída:\n",
        "    custo -- custo da rede\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.size # número de exemplos\n",
        "\n",
        "    # Compute loss from AL and y.\n",
        "    ###Início do código ### (≈ 1 linha de código)\n",
        "    custo = -1/m * np.sum(((Y * (np.log(AL))) + ( (1 - Y) * (np.log(1-AL)))))\n",
        "    ### Fim do código ###\n",
        "    \n",
        "    custo = np.squeeze(custo)      # assegurar o formato experado ( [[17]] para 17).\n",
        "    \n",
        "    \n",
        "    return custo"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhS4QgwnLflH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "52115de1-8637-4852-f230-8711972aa74e"
      },
      "source": [
        "Y, AL = compute_cost_test_case()\n",
        "\n",
        "print(\"custo = \" + str(custo(AL, Y)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "custo = 0.2797765635793422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_ew5cpfLflL"
      },
      "source": [
        "**Valores Esperados**:\n",
        "\n",
        "<table>\n",
        "\n",
        "   <tr>\n",
        "    <td>**custo** </td>\n",
        "    <td> 0.2797765635793422</td> \n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2DIdIlWLflM"
      },
      "source": [
        "## 6 - Fase: Backward propagation (2pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Com funções auxiliares, a fase back propagation é usada para calcular o gradiente da função loss em relação aos parâmetros. \n",
        "\n",
        "**Lembrete**: \n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19lwOgDLuWoMzXjHafCPkj3dzZcSyeh0r)\n",
        "\n",
        "<caption><center> **Figura 3** : <br> *Os blocos roxos representam a fase forward propagation, e os vermelhos representam a fase backward propagation.*  </center></caption>\n",
        "\n",
        "\n",
        "\n",
        "Usaremos duas funções, igualmente feito na fase forward:\n",
        "- LINEAR\n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID"
      ],
      "metadata": {
        "id": "psreXfdrPXu2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THlfUk5eLflM"
      },
      "source": [
        "### 6.1 - Linear backward\n",
        "\n",
        "Para a camada $l$, a parte linear é: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (seguida por uma ativação).\n",
        "\n",
        "Suponha que $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$ já foi calculado. \n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19hp00jo-CAsdBswuxFfLsN0JHbsXl5-1)\n",
        "\n",
        "<caption><center> **Figura 4** </center></caption>\n",
        "\n",
        "As saídas $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ são calculadas usando $dZ^{[l]}$:\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77ryBJfZLflN"
      },
      "source": [
        "**Exercício**: Use as 3 fórmulas acima para implementar linear_backward( )."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbnstv6ZLflN"
      },
      "source": [
        "# linear_backward\n",
        "\n",
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implementa a parte linear da fase backward propagation em uma camada l\n",
        "\n",
        "    Entradas:\n",
        "    dZ -- gradiente do custo em relação a saída linear da camada l\n",
        "    cache -- tupla (A_prev, W, b) vindo da forward propagation da camada l\n",
        "\n",
        "    Saídas:\n",
        "    dA_prev -- gradiente do custo em relação a ativação da camada l-1,\n",
        "    dW -- gradiente do custo em relação a W  da camada l,\n",
        "    db -- gradiente do custo em relação a b,\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    #print(A_prev.shape[1])\n",
        "    #print(dZ.shape[1])\n",
        "\n",
        "    n=b.size\n",
        "\n",
        "    db=b\n",
        "\n",
        "    ### Início do código ### \n",
        "    dW = 1/m * np.dot(dZ, np.transpose(A_prev)) \n",
        "    for i in range(0,n):\n",
        "      db[[i]]= 1/m * sum(dZ[i])\n",
        "    #db = 1/m * sum(dZ)\n",
        "    dA_prev = np.dot(np.transpose(W), dZ)\n",
        "    ### Fim do código ###\n",
        "\n",
        "    print(str(dZ))\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-wWOgAJLflQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eac6ab8e-3c16-4899-ec0c-81b21118e5c1"
      },
      "source": [
        "# Teste\n",
        "dZ, linear_cache = linear_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862]\n",
            " [ 0.86540763 -2.3015387   1.74481176 -0.7612069 ]\n",
            " [ 0.3190391  -0.24937038  1.46210794 -2.06014071]]\n",
            "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
            " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
            " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
            " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
            " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
            "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
            " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
            " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
            "db = [[-0.14713786]\n",
            " [-0.11313155]\n",
            " [-0.13209101]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOHi3L7mLflT"
      },
      "source": [
        "**Valores Esperados**:\n",
        "    \n",
        "```\n",
        "dA_prev = \n",
        " [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
        " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
        " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
        " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
        " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
        "dW = \n",
        " [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
        " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
        " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
        "db = \n",
        " [[-0.14713786]\n",
        " [-0.11313155]\n",
        " [-0.13209101]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWYLQFi7LflT"
      },
      "source": [
        "### 6.2 - Linear-Ativação backward\n",
        "\n",
        "A etapa backward para a ativação **`linear_ativacao_backward`**. \n",
        "\n",
        "Use as funções:\n",
        "- **`sigmoid_backward`**: backward propagation para SIGMOID:\n",
        "\n",
        "```python\n",
        "dZ = sigmoid_backward(dA, ativacao_cache)\n",
        "```\n",
        "\n",
        "- **`relu_backward`**: backward propagation para RELU:\n",
        "\n",
        "```python\n",
        "dZ = relu_backward(dA, ativacao_cache)\n",
        "```\n",
        "\n",
        "Se $g(.)$ é a função de ativação, \n",
        "`sigmoid_backward` e `relu_backward` calcula $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlZnbuWnLflU"
      },
      "source": [
        "# linear_ativacao_backward\n",
        "\n",
        "def linear_ativacao_backward(dA, cache, ativacao):\n",
        "    \"\"\"\n",
        "    Implementa a backward propagation para ativação.\n",
        "    \n",
        "    Entradas:\n",
        "    dA -- gradiente da pos-ativacao gradient para camada l \n",
        "    cache -- tupla de valores (linear_cache, ativacao_cache)\n",
        "    ativacao -- \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Saídas:\n",
        "    dA_prev -- gradiente do custo em relação a ativação da camada l-1,\n",
        "    dW -- gradiente do custo em relação a W  da camada l,\n",
        "    db -- gradiente do custo em relação a b,\n",
        "    \"\"\"\n",
        "    linear_cache, ativacao_cache = cache\n",
        "    \n",
        "    if ativacao == \"relu\":\n",
        "        ### Início do código ### \n",
        "        dZ =  relu_backward(dA, ativacao_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        ### Fim do código ###\n",
        "        \n",
        "    elif ativacao == \"sigmoid\":\n",
        "        ### Início do código ### \n",
        "        dZ = sigmoid_backward(dA, ativacao_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        ### Fim do código ###\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG5XV0YHLflZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e2d70335-d437-4aa1-dc17-4ee3c70b79c1"
      },
      "source": [
        "# Teste\n",
        "\n",
        "dAL, linear_ativacao_cache = linear_activation_backward_test_case()\n",
        "\n",
        "dA_prev, dW, db = linear_ativacao_backward(dAL, linear_ativacao_cache, ativacao = \"sigmoid\")\n",
        "print (\"sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db) + \"\\n\")\n",
        "\n",
        "dA_prev, dW, db = linear_ativacao_backward(dAL, linear_ativacao_cache, ativacao = \"relu\")\n",
        "print (\"relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.10414453 -0.01044791]]\n",
            "sigmoid:\n",
            "dA_prev = [[ 0.11017994  0.01105339]\n",
            " [ 0.09466817  0.00949723]\n",
            " [-0.05743092 -0.00576154]]\n",
            "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
            "db = [[-0.05729622]]\n",
            "\n",
            "[[-0.41675785  0.        ]]\n",
            "relu:\n",
            "dA_prev = [[ 0.44090989  0.        ]\n",
            " [ 0.37883606  0.        ]\n",
            " [-0.2298228   0.        ]]\n",
            "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
            "db = [[-0.20837892]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJWts-SALflc"
      },
      "source": [
        "**Valores esperados com:**\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td > dA_prev </td> \n",
        "           <td >[[ 0.11017994  0.01105339]\n",
        " [ 0.09466817  0.00949723]\n",
        " [-0.05743092 -0.00576154]] </td> \n",
        "\n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > dW </td> \n",
        "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > db </td> \n",
        "           <td > [[-0.05729622]] </td> \n",
        "  </tr> \n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUZv2-aPLfld"
      },
      "source": [
        "**Valores esperados com relu:**\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <td > dA_prev </td> \n",
        "           <td > [[ 0.44090989  0.        ]\n",
        " [ 0.37883606  0.        ]\n",
        " [-0.2298228   0.        ]] </td> \n",
        "\n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > dW </td> \n",
        "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > db </td> \n",
        "           <td > [[-0.20837892]] </td> \n",
        "  </tr> \n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8OsrFC1Lfld"
      },
      "source": [
        "### 6.3 - L-Modelo Backward \n",
        "\n",
        "A Figura mostra a fase backward. \n",
        "\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19dmT5_6jwKXm92AjDB0F-Z0VxI-RoQYH)\n",
        "<caption><center>  **Figura 5** : Fase Backward  </center></caption>\n",
        "\n",
        "**Inicializando a fase backpropagation**:\n",
        "A saída da rede é, \n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. Então temos que calcualar `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$:\n",
        "\n",
        "`dAL`  $=\\frac{Y}{AL} - \\frac{1-Y}{1-Al}$ \n",
        "\n",
        "\n",
        "O gradiente `dAL` para continuar propagando. Como visto na Figura 5, `dAL` vai alimentar a linear_ativacao_backward com ativação SIGMOID (que utilizará os valores armazenados em cache armazenados pela função L_modelo_forward). Depois disso, você terá que usar um loop `for` para percorrer todas as outras camadas usando linear_ativacao_backward com ativação RELU. Você deve armazenar cada dA, dW e db no dicionário grads.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVgJXRPHLfle"
      },
      "source": [
        "# L_modelo_backward\n",
        "\n",
        "def L_modelo_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implementa a backward propagation para [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
        "    \n",
        "    \n",
        "    Entradas:\n",
        "    AL -- Probabiliade de predição da rede, saída da fase forward propagation (L_modelo_forward())\n",
        "    Y -- Vetor de rótulos dos exemplos de treinamento  ( 0 se não tem gato, 1 tem gato )\n",
        "    caches -- lista de caches contendo:\n",
        "                todos cache da linear_ativacao_forward() com \"relu\" ( caches[l], l = 0...L-2)\n",
        "                o cache da linear_ativacao_forward() com \"sigmoid\" (caches[L-1])\n",
        "    \n",
        "    Saídas:\n",
        "    grads -- Um dicionário com os gradientes\n",
        "              \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # número de camadas\n",
        "    m = AL.shape[1] # número de exemplos\n",
        "    \n",
        "    Y = Y.reshape(AL.shape) # Y deve ter o mesmo formato que AL\n",
        "    \n",
        "    # Inicilizando a fase backpropagation\n",
        "    ### Início do código ### \n",
        "    dAL = None # gradiente do custo em relação a AL\n",
        "    ### Fim do código ###\n",
        "    \n",
        "    # gradiente da l-ésima camada (SIGMOID -> LINEAR). \n",
        "    # Entrada: \"dAL, corrente_cache\". Saida: \"d(AL-1), dWL, dbL\"\n",
        "    ### Início do código ### \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = None\n",
        "    ### Fim do código ###\n",
        "    \n",
        "    # Gradientes das camadas anterios: (RELU -> LINEAR)\n",
        "    # Entradas: \"dA(l+1), corrente_cache\". \n",
        "    # Saídas: \"dA(l), dW(l+1), db(l+1)\" \n",
        "    ### Início do código ### \n",
        "    # Loop de l=L-2 até l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "      current_cache = caches[l]\n",
        "      dA_prev_temp, dW_temp, db_temp = None # dice: ver linear_activation_backward()\n",
        "      grads[\"dA\" + str(l)] = None\n",
        "      grads[\"dW\" + str(l + 1)] = None\n",
        "      grads[\"db\" + str(l + 1)] = None\n",
        "    ### Fim do código ###\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjlRs9F0Lflh"
      },
      "source": [
        "AL, Y_teste, caches = L_modelo_backward_teste()\n",
        "\n",
        "grads = L_modelo_backward(AL, Y_teste, caches)\n",
        "print_grads(grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA-YLjT7Lflj"
      },
      "source": [
        "**Valores esperados**\n",
        "\n",
        "<table style=\"width:60%\">\n",
        "  \n",
        "  <tr>\n",
        "    <td > dW1 </td> \n",
        "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
        " [ 0.          0.          0.          0.        ]\n",
        " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > db1 </td> \n",
        "           <td > [[-0.22007063]\n",
        " [ 0.        ]\n",
        " [-0.02835349]] </td> \n",
        "  </tr> \n",
        "  \n",
        "  <tr>\n",
        "  <td > dA1 </td> \n",
        "           <td > [[ 0.12913162 -0.44014127]\n",
        " [-0.14175655  0.48317296]\n",
        " [ 0.01663708 -0.05670698]] </td> \n",
        "\n",
        "  </tr> \n",
        "</table>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvZW8iNXLflk"
      },
      "source": [
        "### 6.4 - Atualização dos parâmetros\n",
        "\n",
        "Usando gradiente descendente: \n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "onde $\\alpha$ é a taxa de aprendizagem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS1_5sdXLflk"
      },
      "source": [
        "**Instruções**:\n",
        "Atualização dos parâmetros usando gradiente descendente: $W^{[l]}$ and $b^{[l]}$ para $l = 1, 2, ..., L$. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwPZN1oQLfll"
      },
      "source": [
        "# atualize_parametros\n",
        "\n",
        "def atualize_parametros(parametros, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Atualização dos parâmetros usando gradiente descendente:\n",
        "    \n",
        "    Entradas:\n",
        "    parametros -- python dicionario contendo os parametros \n",
        "    grads -- python dicionario contendo os gradientes, saída L_modelo_backward\n",
        "    \n",
        "    Saídas:\n",
        "    parametros -- python dicionario contendo os parametros \n",
        "                \n",
        "    \"\"\"\n",
        "    \n",
        "    L = None  # número de camadas da rede\n",
        "\n",
        "    # Atualiza os parametros.\n",
        "    ### Início do código ###\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = None\n",
        "        parameters[\"b\" + str(l+1)] = None\n",
        "    ### Fim do código ###\n",
        "    return parametros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzlnOk-6Lflo"
      },
      "source": [
        "parametros, grads = update_parameters_test_case()\n",
        "parametros = atualize_parametros(parametros, grads, 0.1)\n",
        "\n",
        "print (\"W1 = \"+ str(parametros[\"W1\"]))\n",
        "print (\"b1 = \"+ str(parametros[\"b1\"]))\n",
        "print (\"W2 = \"+ str(parametros[\"W2\"]))\n",
        "print (\"b2 = \"+ str(parametros[\"b2\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYpdW4jtLflr"
      },
      "source": [
        "**Valores esperados**:\n",
        "\n",
        "<table style=\"width:100%\"> \n",
        "    <tr>\n",
        "    <td > W1 </td> \n",
        "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
        " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
        " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > b1 </td> \n",
        "           <td > [[-0.04659241]\n",
        " [-1.28888275]\n",
        " [ 0.53405496]] </td> \n",
        "  </tr> \n",
        "  <tr>\n",
        "    <td > W2 </td> \n",
        "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
        "  </tr> \n",
        "  \n",
        "   <tr>\n",
        "    <td > b2 </td> \n",
        "           <td > [[-0.84610769]] </td> \n",
        "  </tr> \n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNAdvsTALfls"
      },
      "source": [
        "## 7 - Construa o modelo (2pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemente o modelo usando as funções anteriores para treinar os parâmetros da rede no conjunto de dados."
      ],
      "metadata": {
        "id": "LSxQEG3BPl_5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEf7wROhLflu"
      },
      "source": [
        "# L_layer_modelo\n",
        "\n",
        "def L_layer_modelo(X, Y, camada_dims, learning_rate = 0.0075, num_iter = 3000, print_custo=False):\n",
        "    \"\"\"\n",
        "    Implementa a uma rede neural com L-camadas: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Entradas:\n",
        "    X -- conjunto de treinamento representado por uma matriz numpy da forma (num_px * num_px * 3, numero de exemplos)\n",
        "    Y -- rótulos de treinamento representados por uma matriz numpy (vetor) da forma (1, numero de exemplos)\n",
        "    camadas_dims -- lista contendo a dimensão dos dados de entrada e tamanho de cada camada da rede, (numero de camadas + 1).\n",
        "    learning_rate -- lhiperparâmetro que representa a taxa de aprendizado usada na regra de atualização do gradiente descendete\n",
        "    num_iter -- hiperparâmetro que representa o número de iterações para otimizar os parâmetros\n",
        "    print_custo -- imprime o custo a cada 100 iterações\n",
        "    \n",
        "    Saida:\n",
        "    parametros -- parametros aprendidos do modelo.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    custos = []                         # guarda o custo\n",
        "    \n",
        "    # Inicialização dos parametros\n",
        "    ### Início do código ###\n",
        "    parameters = None # dica : ver sua função de inicializacao\n",
        "    ### Fim do código ###\n",
        "    \n",
        "    # Gradiente descendente. Dica : use as funções que você escreveu acima\n",
        "    for i in range(0, num_iter):\n",
        "\n",
        "        # Fase Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### Início do código ###\n",
        "        AL, caches = None\n",
        "        ### Fim do código ###\n",
        "        \n",
        "        # Calculo do Custo.\n",
        "        ### Início do código ###\n",
        "        cost = None\n",
        "        ### Fim do código ###\n",
        "    \n",
        "        # Fase Backward propagation.\n",
        "        ### Início do código ###\n",
        "        grads = None\n",
        "        ### Fim do código ###\n",
        " \n",
        "        # Atualização dos parametros.\n",
        "        ### Início do código ###\n",
        "        parameters = None\n",
        "        ### Fim do código ###\n",
        "                \n",
        "        # Imprime o custo cada 100 iterações\n",
        "        if print_custo and i % 100 == 0:\n",
        "            print (\"Custo depois da iteração %i: %f\" %(i, cost))\n",
        "        if print_custo and i % 100 == 0:\n",
        "            custos.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(custos))\n",
        "    plt.ylabel('custo')\n",
        "    plt.xlabel('iterações (por centenas)')\n",
        "    plt.title(\"Taxa de aprendizagem =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parametros"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwJRf0vlLflx"
      },
      "source": [
        "## 8- Pronto! (1pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Pre-processamento dos dados\n",
        "\n",
        "Vamos construir o modelo para treinar um classificador de imagens (o mesmo da regressão logística)"
      ],
      "metadata": {
        "id": "ANAc11_zPo_-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YUqWglPXDq8"
      },
      "source": [
        "# Lendo os dados (gato/não-gato)\n",
        "def load_dataset():\n",
        "\n",
        "  train_dataset = h5py.File('/<caminho para os dados>/train_catvnoncat.h5', \"r\")\n",
        "  train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
        "  train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
        "\n",
        "  test_dataset = h5py.File('/<caminho para os dados>/test_catvnoncat.h5', \"r\")\n",
        "  test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
        "  test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
        "\n",
        "  classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
        "  train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
        "  test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
        "  \n",
        "  return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d2fWOiJLfly"
      },
      "source": [
        "# Lendo os dados (gato/não-gato)\n",
        "treino_x_orig, treino_y, teste_x_orig, teste_y, classes = load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNsogn4bLfl1"
      },
      "source": [
        "Pre-processamento necessário.\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=1zCnEB2rwc4lXU_7RTS4TXhqCwsJubg7H)\n",
        "\n",
        "<caption><center> <u>Figura 6</u>: Vetorização de uma imagem. <br> </center></caption>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcIv5zX9Lfl1"
      },
      "source": [
        "\n",
        "m_treino = len(treino_x_orig)\n",
        "m_teste = len(teste_x_orig)\n",
        "num_px = teste_x_orig[1].shape[1]\n",
        "\n",
        "#  Vetorizando as imagens de treinamento e teste \n",
        "\n",
        "### Início do código ###\n",
        "None # dica : utilize reshape para mudar o formato dos dados\n",
        "### Fim do código ###\n",
        "\n",
        "### Início do código ###\n",
        "# Normalize os dados para ter valores de recurso entre 0 e 1.\n",
        "None\n",
        "### Fim do código ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOzgbH4-Lfl3"
      },
      "source": [
        "\n",
        "### Testando com rede neural com 2 camadas\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19mux_FFpeZkj5YiV51bNE2CK3i2nBSau)\n",
        "<caption><center> <u>Figura 7</u>: Rede neural com 2 camadas. <br> Resumo do modelo: ***ENTRADA -> LINEAR -> RELU -> LINEAR -> SIGMOID -> SAIDA***. </center></caption>\n",
        "\n",
        "<!--\n",
        "<u>Detailed Architecture of figure 2</u>:\n",
        "- The input is a (64,64,3) image which is flattened to a vector of size $(12288,1)$. \n",
        "- The corresponding vector: $[x_0,x_1,...,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
        "- You then add a bias term and take its relu to get the following vector: $[a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T$.\n",
        "- You then repeat the same process.\n",
        "- You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias). \n",
        "- Finally, you take the sigmoid of the result. If it is greater than 0.5, you classify it to be a cat.\n",
        "!-->"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKd_8OcsLfl4"
      },
      "source": [
        "### Executar uma rede de 2 camada ###\n",
        "camadas_dims = [12288, 7, 1] \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1UpnIDtLfl7"
      },
      "source": [
        "## Treine a rede\n",
        "parametros = L_layer_modelo(treino_x, treino_y, camada_dims, num_iter = 2500, print_custo=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL39tGwTLfl9"
      },
      "source": [
        "**Use os parâmetros treinados** para classificar as imagens de treinamento e teste e verificar a acurácia. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEehDg8DLfl_"
      },
      "source": [
        "## Predição da rede\n",
        "# dica : re-utilize e modifique a FUNCAO de predição do Lab1B\n",
        "\n",
        "None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otlPGXyzBRuP"
      },
      "source": [
        "Resultado esperado:\n",
        "\n",
        "Acurácia treino = 100%\n",
        "\n",
        "Acurácia teste = 72%\n",
        "\n",
        "Por que você obteve 100% no treino e apenas 72% no teste?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FN1QQO4LfmB"
      },
      "source": [
        "### Testando com uma rede com 4 camadas\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=19h9LuWkWLVMYgAAoQKTfjJ-Er-tlw8En)\n",
        "<caption><center> <u>Figura 8</u>: Rede neural com L camadas. <br> Resumo do modelo: ***ENTRADA -> LINEAR -> RELU -> LINEAR -> SIGMOID -> SAIDA***. </center></caption>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyZpNGpcLfmC"
      },
      "source": [
        "### Executar uma rede de 4 camada ###\n",
        "camadas_dims = [12288, 20, 7, 5, 1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5aA5aR1LfmF"
      },
      "source": [
        "## Treine a rede\n",
        "parametros = L_layer_modelo(treino_x, treino_y, camada_dims, num_iter = 2500, print_custo=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVRI71VlLfmI"
      },
      "source": [
        "**Use os parâmetros treinados** para classificar as imagens de treinamento e teste e verificar a acurácia. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf8WzSsULfmI"
      },
      "source": [
        "## Predição da rede\n",
        "# dica : re-utilize e modifique a FUNCAO de predição do Lab1B\n",
        "y_pred = None\n",
        "\n",
        "# dica2: compute a matirz de confusão"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qg857Y9-Z7W"
      },
      "source": [
        "Resultado esperado:\n",
        "\n",
        "Acurácia no treino: 0.6555023923444976 \n",
        "\n",
        "Acurácia no teste: 0.34\n",
        "\n",
        "Este resultado foi melhor ou pior do que com duas camadas? Tente explicar os motivos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luBAEzfiS7x8"
      },
      "source": [
        "# **Parte 2** - Classificação de múltiplas classes e uso de frameworks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No exemplo anterior, usamos uma arquitetura para classificação binária. Para classificaçõ de múltiplas classes, tem-se um neurônio de saída para cada classe (como ilustrado no exemplo da Figura 9) e deve-se usar a operação Softmax antes de se calcular o custo (entropia cruzada ou cross-entropy como no exemplo anterior). Consute o capítulo [3.6 do livro](http://d2l.ai/chapter_linear-networks/softmax-regression-scratch.html) para entender melhor.  No caso de se usar softmax, deve-se usar a função *one_hot* para transformar a saída em logits. Veja a função *one_hot* fornecida. Ela transforma um escalar em um *hot encoder*, de acordo com o número de classes.\n",
        "\n",
        "![Arq,widht=10](https://drive.google.com/uc?export=view&id=1WV_4AT49bYcqsp6PB0FoO4p-gASo0bjL)<caption><center> <u>Figura 9</u>: Rede neural dois neurônios de saída. <br> </center></caption>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uOdSWNaHRnKn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcJ7w8v1WBUx"
      },
      "source": [
        "# nclasses : numero de classes do prolema, y : um escalar ou vetor de escalares\n",
        "def one_hot(n_classes, y):\n",
        "    return np.eye(n_classes)[y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcMQfNh4WEKO"
      },
      "source": [
        "# ToDo : execute o exemplo e veja o resultado para 4 escalares no vetor de variáveis dependentes\n",
        "one_hot(n_classes=10, y=[0, 4, 9, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54dwXMXHWTCt"
      },
      "source": [
        "### Função softmax \n",
        "\n",
        "A função softmax transforma a saĩda em uma distribuição de probabilidades. Assim, a soma de todas as saídas dos neurônio da última camada sempre vai ser igual a 1:\n",
        "\n",
        "$$\n",
        "softmax(\\mathbf{x}) = \\frac{1}{\\sum_{i=1}^{n}{e^{x_i}}}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "  e^{x_1}\\\\\\\\\n",
        "  e^{x_2}\\\\\\\\\n",
        "  \\vdots\\\\\\\\\n",
        "  e^{x_n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "o gradiente para o custo usando-se a função softmax é trivial de se calcular:\n",
        "\n",
        "$$dw = softmax(\\mathbf{y_{pred}}) - y$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XKZaXQUWXTE"
      },
      "source": [
        "def softmax(X):\n",
        "    exp = np.exp(X)\n",
        "    return exp / np.sum(exp, axis=-1, keepdims=True)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovN7neApWfel",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8067cb1b-439e-41be-998a-e3e15036d2ab"
      },
      "source": [
        "# ToDo : teste sua função softmax com a instância do exemplo abaixo \n",
        "print(softmax([10, 2, -3]))\n",
        "\n",
        "# As saídas individuais devem ser entre 0 e 1 de forma que a soma seja 1. lembre-se, com softmax, temos probabilidades.\n",
        "print(np.sum(softmax([10, 2, -3])))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQJhGae7Wqpr"
      },
      "source": [
        "Perceba que nosso código também funciona se você passar um lote (batch) de amostras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AVEyZmQWpJx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6a6c4b50-251a-41b5-cb48-c25988888bfe"
      },
      "source": [
        "# Veja a saída abaixo\n",
        "X = np.array([[10, 2, -3],\n",
        "              [-1, 5, -20]])\n",
        "print(softmax(X))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[9.99662391e-01 3.35349373e-04 2.25956630e-06]\n",
            " [2.47262316e-03 9.97527377e-01 1.38536042e-11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_plFTAbuW64u"
      },
      "source": [
        "Em seguida, deve-se computar o erro entre um vetor predito Y_pred e o vetor de rótulos Y_true. para tal, deve-se usar cross entropy loss, ou verossimilhança negativa (negative log likelihood). A função cross_entrppy() implementa a verossimilhança negativa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzgQbzRlW5WU"
      },
      "source": [
        "def cross_entropy(Y_true, Y_pred):\n",
        "    EPSILON = 1e-8\n",
        "\n",
        "    Y_true, Y_pred = np.atleast_2d(Y_true), np.atleast_2d(Y_pred)\n",
        "    loglikelihoods = np.sum(np.log(EPSILON + Y_pred) * Y_true, axis=1)\n",
        "    \n",
        "    return -np.mean(loglikelihoods)"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZBY_8TGXAB0"
      },
      "source": [
        "verifique o erro de uma predição bem ruim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H78KTS8MXB8L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d004c1d4-fec4-43ab-96a8-c86a10bac984"
      },
      "source": [
        "print(cross_entropy([1, 0, 0], softmax([0.12, 4, 10])))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.882330913250298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSEH_4N5XE6r"
      },
      "source": [
        "verifique o erro de uma boa predição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXArje6XXGi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b2509bc9-5af2-4417-8eb2-c9ced9317e8b"
      },
      "source": [
        "print(cross_entropy([1, 0, 0], [0.98, 0.01, .01]))"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.020202697113437834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bf2ChgHXJjT"
      },
      "source": [
        "A função cross_entropy() também deve funcionar para um lote de dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx1hEVroXMXx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0e677270-d30e-44ca-e61f-90b22226223f"
      },
      "source": [
        "# Verifique a cross-entropy das três amostas seguintes:\n",
        "\n",
        "Y_true = np.array([[0, 1, 0],\n",
        "                   [1, 0, 0],\n",
        "                   [0, 0, 1]])\n",
        "\n",
        "Y_pred = np.array([[0,   1,    0],\n",
        "                   [.99, 0.01, 0],\n",
        "                   [0,   0,    1]])\n",
        "\n",
        "# repare que as amostras são praticamente predições perfeitas\n",
        "print(cross_entropy(Y_true, Y_pred))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0033501019174971905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0qdtdxIZtUb"
      },
      "source": [
        "## Pré-processamento dos dados\n",
        "\n",
        "Vamos usar a biblioteca scikit learn para nos auxiliar na execução da prática. \n",
        "Veja a documentação em https://scikit-learn.org/stable/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDJpmPmhZx3_"
      },
      "source": [
        "Considere a base de dados abaixo. Ela é referente a um atividade em um site de vendas qualquer. O objetivo com esta base é tentar predizer quais clientes futuros terão probabildiade de comprar algum produto, com base em algumas características, como cidade em que mora, idade e salário.\n",
        "\n",
        "*Carregando os dados*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShTbKDBbZ0ml"
      },
      "source": [
        "# Importe as bibliotecas NumPy, Pandas e Matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# carregue os dados do arquivo e armazenar em um Dataframe\n",
        "dataset = pd.read_csv('<caminho>/datasets/Data.csv')\n",
        "\n",
        "# imprima a estrutura dataset\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X189fCBZ4Ib"
      },
      "source": [
        "Crie dois objetos, uma chamado X, para receber as caracteísticas das instâncias e um chamado y para receber as classes. Observe que as instâncias devem ser organizadas em linha Assm, as características da linha 0 de X devem corresponder a classe da linha 0 de y Podemos chamar as variáveis de X (ou características -usadas para fazer a predição) de variáveis independentes e a variável de y (classe a ser predita) de variável dependente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ODN_jkZ7sr"
      },
      "source": [
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, 3].values\n",
        "\n",
        "# imprima X e y\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "# imprima e analise o formato dos objetos\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAxJX2xkaB23"
      },
      "source": [
        "from sklearn.preprocessing import Imputer\n",
        "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
        "imputer.fit(X[:, 1:3])\n",
        "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
        "\n",
        "# imprima a nova matriz X\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}